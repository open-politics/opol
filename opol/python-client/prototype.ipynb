{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPOL Python Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# 1. Setup client (using opol.io)\n",
    "# opol = OPOL(api_key=\"\")\n",
    "\n",
    "# Alternatively - if you have booted the opol stack on your machine (no api key needed)\n",
    "# Read opol/opol/stack/Readme.md for more information\n",
    "opol = OPOL(mode=\"local\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "opol = OPOL(mode=\"local\", api_key=\"\")\n",
    "\n",
    "# Class submethods default to search\n",
    "print(opol.articles.get_articles(query=\"apple\"))\n",
    "print(opol.articles(\"New York\", pretty=True))    \n",
    "\n",
    "# Use date\n",
    "articles = opol.articles.get_articles(\"Berlin\", limit=100)\n",
    "\n",
    "entities = [article['title'] for article in articles[:3]]\n",
    "entity_ids = [article['id'] for article in articles]\n",
    "print(entities)\n",
    "print(entity_ids)\n",
    "\n",
    "geojson = opol.geo.by_id(entity_ids)\n",
    "print(geojson[0]['features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "opol = OPOL(mode=\"local\", api_key=\"\")\n",
    "\n",
    "articles = opol.articles.by_entity(\"Trump\", limit=100)\n",
    "\n",
    "for article in articles:\n",
    "    print(article.keys())\n",
    "    print(f\"{article['title']}:\" + article['insertion_date'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geojson & Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "import os\n",
    "\n",
    "# set env variable\n",
    "os.environ['OPOL_MODE'] = \"remote\"\n",
    "\n",
    "opol = OPOL(api_key=\"\")\n",
    "\n",
    "geojson = opol.geo.json_by_event(\"War\", limit=5)\n",
    "print(geojson)\n",
    "\n",
    "berlin_coords = opol.geo.code(\"Berlin\")[\"coordinates\"]\n",
    "print(berlin_coords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "from pprint import pprint\n",
    "\n",
    "opol = OPOL(mode=\"local\", api_key=\"\")\n",
    "\n",
    "# polls = opol.scraping.polls(\"Germany\")\n",
    "\n",
    "# for pol in polls:\n",
    "#     if pol['party'] == \"GRÃœNE\" or pol['party'] == \"CDU/CSU\":\n",
    "#         print(pol)\n",
    "\n",
    "\n",
    "# Latest Polls for each Insittute\n",
    "# polls = opol.scraping.polls(\"Germany\", latest=True)\n",
    "\n",
    "# for poll in polls:\n",
    "#     print(poll)\n",
    "\n",
    "ranked_polls = opol.scraping.polls(\"Germany\", summarised=True)\n",
    "\n",
    "for poll in ranked_polls:\n",
    "    print(poll['party'] + \": \" + str(poll['percentage']) + f\" ({poll['change_since_election']})\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legislation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "\n",
    "opol = OPOL(api_key=\"\")\n",
    "\n",
    "events = opol.scraping.legislation(\"Germany\")\n",
    "\n",
    "\n",
    "for event in events[:1]:\n",
    "    print(event)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "\n",
    "opol = OPOL(mode=\"local\")\n",
    "\n",
    "all_events = opol.scraping.economic(\"Germany\")\n",
    "\n",
    "gdp_events = opol.scraping.economic(\"Germany\", indicators=[\"GDP\"])\n",
    "\n",
    "\n",
    "for event in all_events[:3]:\n",
    "    print(event)\n",
    "\n",
    "print(\"*\"*20)\n",
    "\n",
    "for event in gdp_events[:3]:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Classifications (custom llm wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.api.classification import Classification\n",
    "from opol.main import OPOL\n",
    "import os\n",
    "\n",
    "# Initialize the OPOL client in local mode\n",
    "opol = OPOL(mode=\"local\")\n",
    "\n",
    "# Set the API key for accessing the Google Generative AI service\n",
    "# api_key = \"\"\n",
    "api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "\n",
    "# Initialize the classification service with the specified provider, model, and API key\n",
    "xclass = opol.classification(provider=\"Google\", model_name=\"models/gemini-1.5-flash-latest\", llm_api_key=api_key)\n",
    "\n",
    "# Example 1: Classify user preference for frozen dairy products on a scale of 1 to 10\n",
    "user_input = \"I love ice cream\"\n",
    "prompt = \"On a 1-10 scale how much the user likes frozen dairy products\"\n",
    "int_value = xclass.classify(\"int\", prompt, user_input)\n",
    "print(\"User preference rating:\", int_value)\n",
    "\n",
    "# Example 2: Extract keywords from a given text\n",
    "text = \"In the madagascan wilds the biggest wild animal is the elephant in the globe finance trade is offshore jusrisdiction diffusion\"\n",
    "instruction = \"The topics relevant to this text. Only semantically relevant to a content system\"\n",
    "keywords = xclass.classify(\"List[str]\", instruction, text)\n",
    "print(\"Extracted keywords:\", keywords)\n",
    "\n",
    "# Example 3: Use a Pydantic model to classify request types with keywords and relevance level\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class RequestType(BaseModel):\n",
    "    \"\"\"\n",
    "    This is the main classification for incoming request types.\n",
    "    \"\"\"\n",
    "    keywords: List[str] = Field(description=\"The keywords relevant to this text. Only semantically relevant to a content system\")\n",
    "    relevance_level: int = Field(description=\"On a 1-10 scale how much the content is relevant\")\n",
    "\n",
    "# Classify the request using the Pydantic model\n",
    "request = xclass.classify(RequestType, instruction, text)\n",
    "print(\"Classified request:\", request)\n",
    "\n",
    "## Results\n",
    "# User preference rating: 10\n",
    "# Extracted keywords: ['madagascan', 'wilds', 'biggest', 'animal', 'elephant']\n",
    "# Classified request: keywords=['madagascar', 'wildlife', 'elephant'] relevance_level=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# 1. Setup client (using opol.io)\n",
    "# opol = OPOL(api_key=\"\")\n",
    "\n",
    "# Alternatively - if you have booted the opol stack on your machine (no api key needed)\n",
    "# Read opol/opol/stack/Readme.md for more information\n",
    "opol = OPOL(mode=\"local\")\n",
    "\n",
    "\n",
    "for poll in opol.scraping.polls(\"Germany\", latest=True):\n",
    "    print(poll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engine (Google, Bing, Wikipedia, Axiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "opol = OPOL(mode=\"local\")\n",
    "\n",
    "articles = opol.search.engine(\"Latest news from Gaza\", engine=\"wikipedia\")\n",
    "\n",
    "\n",
    "for article in articles[:2]:\n",
    "    print(article)\n",
    "\n",
    "print(\"*\"*20)\n",
    "images = opol.search.image(\"Latest news from Gaza\")\n",
    "\n",
    "for image in images[:6]:\n",
    "    print(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "\n",
    "opol = OPOL(mode=\"local\")\n",
    "\n",
    "embeddings = opol.embeddings(\"What is the capital of Germany?\")\n",
    "\n",
    "print(embeddings)\n",
    "print(len(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "opol = OPOL(mode=\"local\")\n",
    "api_key = \"\"\n",
    "fastclass = opol.classification(provider=\"Google\", model_name=\"models/gemini-1.5-flash-latest\", llm_api_key=api_key)\n",
    "\n",
    "\n",
    "class ExpandedQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    As a political intelligence analyst, your task is to generate a set of three queries that delve deeper into geopolitical dynamics, building upon the initial query and the insights gathered from its search results.\n",
    "\n",
    "    For instance, if the original query was \"Middle East\", your output should follow this format:\n",
    "\n",
    "    [\n",
    "        \"Current political situation in the Middle East\",\n",
    "        \"Recent military & conflict developments in the Middle East\",\n",
    "        \"Economic assessments of the Middle East\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    Aim to create queries that progressively explore more specific geopolitical factors, implications, or related topics concerning the initial query. The goal is to anticipate the analyst's potential information needs and guide them towards a more comprehensive understanding of the geopolitical landscape.\n",
    "    Please match the language of the response to the analyst's language.\n",
    "    Also return the timeframe you want to search in, depending on the generality of the issue.\n",
    "    Very detailed and specified queries are likely to inquire more recent content.  \n",
    "    \n",
    "    Return three queries.\n",
    "    Return one timeframe, which is a literal of: [ day, month, year ]\n",
    "\n",
    "    \"\"\"\n",
    "    similar: List[str] = None\n",
    "    time_range: str = None\n",
    "\n",
    "query = \"Current developments in Gaza?\"\n",
    "instruction = \"\"\n",
    "expanded_query = fastclass.classify(ExpandedQuery, instruction, query)\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "# Collect articles (max 50)\n",
    "counter = 0\n",
    "for query in expanded_query.similar:\n",
    "    articles = opol.search.engine(query, time_range=expanded_query.time_range)\n",
    "    for article in articles:\n",
    "        if counter > 70:\n",
    "            break\n",
    "        all_articles.append(article)\n",
    "        counter += 1\n",
    "\n",
    "# Rerank articles based on similarity to the query\n",
    "query_embedding = opol.embeddings(query)\n",
    "ranked_articles = opol.embeddings.rerank_articles(query_embedding, all_articles, text_field=\"title\")\n",
    "\n",
    "# Print top 2 ranked articles\n",
    "for article in ranked_articles[:15]:\n",
    "    for field in article[0]:\n",
    "        print(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "\n",
    "opol = OPOL(mode=\"local\")\n",
    "\n",
    "url = \"https://www.dw.com/en//en/pakistans-punjab-imposes-activity-bans-amid-intense-smog/a-70755328\"\n",
    "scraped_url = opol.scraping.url(url)\n",
    "\n",
    "print(scraped_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opol.main import OPOL\n",
    "\n",
    "opol = OPOL(mode=\"remote\", api_key=\"\")\n",
    "\n",
    "geocode = opol.geo.code(\"Berlin\")\n",
    "\n",
    "print(geocode)\n",
    "articles = opol.search.engine(\"Latest news from Gaza\", engine=\"wikipedia\")\n",
    "\n",
    "for article in articles:\n",
    "    print(article)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import regex as r\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_party_and_percentage():\n",
    "    # Define the Wikipedia API URL for the opensearch endpoint\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"opensearch\",\n",
    "        \"search\": \"2021 German federal election\",\n",
    "        \"limit\": 1,\n",
    "        \"namespace\": 0,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    # Perform the search\n",
    "    response = requests.get(search_url, params=search_params)\n",
    "    search_results = response.json()\n",
    "    \n",
    "    # Extract the page URL from the search results\n",
    "    if search_results and len(search_results) > 3 and search_results[3]:\n",
    "        page_url = search_results[3][0]\n",
    "    else:\n",
    "        raise Exception(\"Wikipedia page not found.\")\n",
    "    \n",
    "    # Fetch the page content\n",
    "    page_response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "    \n",
    "    # Find the election results table\n",
    "    table = soup.find('table', {'class': 'infobox'})\n",
    "    if not table:\n",
    "        raise Exception(\"Election results table not found.\")\n",
    "    \n",
    "    # Initialize a dictionary to hold data\n",
    "    data = {\n",
    "        'Party': [],\n",
    "        'Percentage': []\n",
    "    }\n",
    "    \n",
    "    # Iterate over table rows to extract data\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        header = row.find('th')\n",
    "        data_cells = row.find_all('td')\n",
    "        if header and data_cells:\n",
    "            key = header.get_text(strip=True)\n",
    "            if key in data:\n",
    "                for cell in data_cells:\n",
    "                    text = cell.get_text(strip=True)\n",
    "                    if text:\n",
    "                        if key == 'Percentage':\n",
    "                            text = r.search(r'\\d{1,2}\\.\\d{1,2}', text).group(0)\n",
    "                        data[key].append(text)\n",
    "    \n",
    "    # Print the extracted data\n",
    "    for party, percentage in zip(data['Party'], data['Percentage']):\n",
    "        print(f\"Party: {party}, Percentage: {percentage}\")\n",
    "\n",
    "fetch_party_and_percentage()\n",
    "\n",
    "from opol.main import OPOL\n",
    "from pprint import pprint\n",
    "\n",
    "opol = OPOL(mode=\"local\", api_key=\"\")\n",
    "\n",
    "# polls = opol.scraping.polls(\"Germany\")\n",
    "\n",
    "# for pol in polls:\n",
    "#     if pol['party'] == \"GRÃœNE\" or pol['party'] == \"CDU/CSU\":\n",
    "#         print(pol)\n",
    "\n",
    "\n",
    "# Latest Polls for each Insittute\n",
    "# polls = opol.scraping.polls(\"Germany\", latest=True)\n",
    "\n",
    "# for poll in polls:\n",
    "#     print(poll)\n",
    "\n",
    "ranked_polls = opol.scraping.polls(\"Germany\", summarised=True)\n",
    "\n",
    "for poll in ranked_polls:\n",
    "    print(poll['party'] + \": \" + str(poll['percentage']) + f\" ({poll['change_since_election']})\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting single document...\n",
      "Response: message='Documents ingested successfully.' content_ids=['fccef182-f0df-4dbd-95a6-cebf74a5b20a'] contents=None\n",
      "\n",
      "Ingesting multiple documents...\n",
      "Response: message='Documents ingested successfully.' content_ids=['fb8e3f1c-8e03-4e0a-aafc-58b9eb713c0c', 'f98fecb2-82ba-41f1-aec4-799c1229a307'] contents=None\n",
      "[Document(id='fb8e3f1c-8e03-4e0a-aafc-58b9eb713c0c', url='https://example.com/article2', title='Example Article 2', content_type='article', source=None, text_content='Test content 2', insertion_date='2025-01-10T17:30:51.052560+00:00', summary='Test summary 2', meta_summary=None, media_details=None), Document(id='f98fecb2-82ba-41f1-aec4-799c1229a307', url='https://example.com/article3', title='Example Article 3', content_type='article', source=None, text_content='Test content 3', insertion_date='2025-01-10T17:30:51.056195+00:00', summary='Test summary 3', meta_summary=None, media_details=None)]\n",
      "********************\n",
      "id='fb8e3f1c-8e03-4e0a-aafc-58b9eb713c0c' url='https://example.com/article2' title='Example Article 2' content_type='article' source=None text_content='Test content 2' insertion_date='2025-01-10T17:30:51.052560+00:00' summary='Test summary 2' meta_summary=None media_details=None\n",
      "id='f98fecb2-82ba-41f1-aec4-799c1229a307' url='https://example.com/article3' title='Example Article 3' content_type='article' source=None text_content='Test content 3' insertion_date='2025-01-10T17:30:51.056195+00:00' summary='Test summary 3' meta_summary=None media_details=None\n",
      "{'message': 'Document deleted successfully.', 'id': 'fb8e3f1c-8e03-4e0a-aafc-58b9eb713c0c'}\n",
      "{'message': 'Document deleted successfully.', 'id': 'f98fecb2-82ba-41f1-aec4-799c1229a307'}\n"
     ]
    }
   ],
   "source": [
    "from opol.main import OPOL\n",
    "from opol.api.documents import Document\n",
    "\n",
    "def test_documents():\n",
    "    opol = OPOL(mode=\"local\")\n",
    "    docs = opol.documents\n",
    "\n",
    "    # Test single document ingestion\n",
    "    doc = Document(\n",
    "        url=\"https://example.com/article1\",\n",
    "        title=\"Example Article 1\",\n",
    "        text_content=\"Test content 1\",\n",
    "        summary=\"Test summary 1\"\n",
    "    )\n",
    "    print(\"Ingesting single document...\")\n",
    "    response = docs.ingest(doc, overwrite=True)\n",
    "    print(f\"Response: {response}\\n\")\n",
    "\n",
    "    # Test multiple document ingestion\n",
    "    multi_docs = [\n",
    "        Document(\n",
    "            url=\"https://example.com/article2\",\n",
    "            title=\"Example Article 2\", \n",
    "            text_content=\"Test content 2\",\n",
    "            summary=\"Test summary 2\"\n",
    "        ),\n",
    "        Document(\n",
    "            url=\"https://example.com/article3\",\n",
    "            title=\"Example Article 3\",\n",
    "            text_content=\"Test content 3\", \n",
    "            summary=\"Test summary 3\"\n",
    "        )\n",
    "    ]\n",
    "    print(\"Ingesting multiple documents...\")\n",
    "    response = docs.ingest(multi_docs, overwrite=True)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "    read_articles = docs.read(ids=response.content_ids)\n",
    "    print(read_articles)\n",
    "    print(\"*\"*20)\n",
    "    for article in read_articles:\n",
    "        print(article)\n",
    "\n",
    "    for article in read_articles:\n",
    "        delete_response = docs.delete(id=article.id)\n",
    "        print(delete_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
